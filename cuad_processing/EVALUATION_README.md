# CUAD Bedrock Labeler Evaluation

This document describes how to use the `evaluate_labeler.py` script to measure the performance of the bedrock labeler against ground truth CUAD labels.

## Overview

The evaluation script compares predictions from the bedrock labeler against ground truth labels and calculates comprehensive metrics at both the overall and per-category levels.

### Metrics Calculated

#### Overall Metrics
- **Accuracy**: Overall correctness across all predictions
- **Precision** (Macro/Micro/Weighted): Proportion of positive predictions that are correct
- **Recall** (Macro/Micro/Weighted): Proportion of actual positives that are correctly identified
- **F1 Score** (Macro/Micro/Weighted): Harmonic mean of precision and recall
- **Confusion Matrix**: True Positives, True Negatives, False Positives, False Negatives

#### Per-Category Metrics (for all 41 CUAD categories)
- **Accuracy**: Correctness for this specific category
- **Precision**: Proportion of positive predictions that are correct for this category
- **Recall**: Proportion of actual positives correctly identified for this category
- **F1 Score**: Harmonic mean of precision and recall for this category
- **Support**: Number of positive samples in ground truth for this category
- **Confusion Matrix**: TP, TN, FP, FN for this category

## Requirements

```bash
pip install scikit-learn numpy
```

Or install from the requirements file:
```bash
pip install -r requirements.txt
```

## Usage

### Basic Usage

```bash
python evaluate_labeler.py \
    --predictions path/to/predictions.csv \
    --ground-truth path/to/ground_truth.csv
```

This will:
1. Load both CSV files
2. Align the datasets by contract ID
3. Calculate all metrics
4. Print results to console
5. Save results to `./evaluation_results/evaluation_results.json`

### Save Results to Custom Location

```bash
python evaluate_labeler.py \
    --predictions path/to/predictions.csv \
    --ground-truth path/to/ground_truth.csv \
    --output my_results.json
```

### Save Results as CSV

```bash
python evaluate_labeler.py \
    --predictions path/to/predictions.csv \
    --ground-truth path/to/ground_truth.csv \
    --output results.csv \
    --output-format csv
```

This creates two CSV files:
- `results.csv`: Per-category metrics
- `results_overall.csv`: Overall metrics

### Quiet Mode (No Console Output)

```bash
python evaluate_labeler.py \
    --predictions path/to/predictions.csv \
    --ground-truth path/to/ground_truth.csv \
    --output results.json \
    --quiet
```

## Input File Format

Both prediction and ground truth CSV files should follow the CUAD CSV format with 83 columns:
- 1 `contract_id` column
- 41 categories × 2 columns each (text and label) = 82 columns

Example columns:
```
contract_id, document_name_text, document_name_label, parties_text, parties_label, ...
```

### Predictions CSV
Generated by `oss120_bedrock_labeler.py`:
```bash
python oss120_bedrock_labeler.py \
    --input-csv input.csv \
    --output predictions.csv
```

### Ground Truth CSV
Generated by `generate_cuad_csv.py`:
```bash
python generate_cuad_csv.py \
    --input-json data/CUADv1.json \
    --output-csv ground_truth.csv
```

## Output Format

### JSON Output

```json
{
  "overall_metrics": {
    "accuracy": 0.8523,
    "precision_macro": 0.7891,
    "precision_micro": 0.8234,
    "recall_macro": 0.7654,
    "recall_micro": 0.7823,
    "f1_macro": 0.7765,
    "f1_micro": 0.8021,
    "total_samples": 41000,
    "true_positives": 3456,
    "true_negatives": 35234,
    "false_positives": 1234,
    "false_negatives": 1076
  },
  "category_metrics": [
    {
      "category": "Document Name",
      "accuracy": 0.9234,
      "precision": 0.8765,
      "recall": 0.8543,
      "f1_score": 0.8652,
      "support": 234,
      "total_samples": 1000,
      "true_positives": 200,
      "true_negatives": 723,
      "false_positives": 28,
      "false_negatives": 49
    },
    ...
  ],
  "summary": {
    "num_categories": 41,
    "avg_accuracy": 0.8523,
    "avg_precision": 0.7891,
    "avg_recall": 0.7654,
    "avg_f1_score": 0.7765
  }
}
```

### CSV Output

**results.csv** (Per-category metrics):
```csv
category,accuracy,precision,recall,f1_score,support,total_samples,positive_predictions,positive_ground_truth,true_positives,true_negatives,false_positives,false_negatives
Document Name,0.9234,0.8765,0.8543,0.8652,234,1000,228,234,200,723,28,49
Parties,0.8876,0.8234,0.8012,0.8121,456,1000,445,456,365,489,80,66
...
```

**results_overall.csv** (Overall metrics):
```csv
Metric,Value
accuracy,0.8523
precision_macro,0.7891
precision_micro,0.8234
recall_macro,0.7654
recall_micro,0.7823
f1_macro,0.7765
f1_micro,0.8021
total_samples,41000
true_positives,3456
true_negatives,35234
false_positives,1234
false_negatives,1076
```

## Console Output Example

```
================================================================================
CUAD BEDROCK LABELER EVALUATION RESULTS
================================================================================

--------------------------------------------------------------------------------
OVERALL METRICS
--------------------------------------------------------------------------------
Total Samples: 41,000
Positive Predictions: 4,690
Positive Ground Truth: 4,532

Accuracy:           0.8523

Precision (Macro):  0.7891
Precision (Micro):  0.8234
Precision (Weighted): 0.8156

Recall (Macro):     0.7654
Recall (Micro):     0.7823
Recall (Weighted):  0.7745

F1 Score (Macro):   0.7765
F1 Score (Micro):   0.8021
F1 Score (Weighted): 0.7943

Confusion Matrix:
  True Positives:  3,456
  True Negatives:  35,234
  False Positives: 1,234
  False Negatives: 1,076

--------------------------------------------------------------------------------
PER-CATEGORY METRICS
--------------------------------------------------------------------------------
Category                                        Acc   Prec    Rec     F1   Supp
--------------------------------------------------------------------------------
Document Name                                 0.923  0.877  0.854  0.865    234
Parties                                       0.888  0.823  0.801  0.812    456
Agreement Date                                0.845  0.789  0.767  0.778    189
...
--------------------------------------------------------------------------------
Average                                       0.852  0.789  0.765  0.777
Std Dev                                       0.045  0.067  0.072  0.068
================================================================================
```

## Understanding the Metrics

### Accuracy
- **Definition**: (TP + TN) / (TP + TN + FP + FN)
- **Range**: 0.0 to 1.0 (higher is better)
- **Interpretation**: Overall correctness of predictions

### Precision
- **Definition**: TP / (TP + FP)
- **Range**: 0.0 to 1.0 (higher is better)
- **Interpretation**: Of all positive predictions, how many were correct?
- **Macro**: Average precision across all categories (treats all categories equally)
- **Micro**: Global precision across all predictions (treats all samples equally)
- **Weighted**: Weighted average by support (number of true instances per category)

### Recall
- **Definition**: TP / (TP + FN)
- **Range**: 0.0 to 1.0 (higher is better)
- **Interpretation**: Of all actual positives, how many were correctly identified?
- **Macro/Micro/Weighted**: Same averaging methods as precision

### F1 Score
- **Definition**: 2 × (Precision × Recall) / (Precision + Recall)
- **Range**: 0.0 to 1.0 (higher is better)
- **Interpretation**: Harmonic mean of precision and recall (balanced metric)

### Support
- **Definition**: Number of positive samples in ground truth for a category
- **Interpretation**: How many contracts actually contain this category

## Common Issues and Solutions

### Issue: No common contract IDs found

**Error Message:**
```
ValueError: No common contract IDs found between predictions and ground truth!
```

**Solution:**
- Ensure both CSV files use the same `contract_id` format
- Check that you're comparing the correct files (e.g., test set predictions vs. test set ground truth)
- Verify that both files were generated from the same source data

### Issue: Missing categories in predictions

**Behavior:**
- Missing categories are treated as label=0 (not present)
- The script will still calculate metrics for all 41 categories

**Solution:**
- This is expected behavior if the bedrock labeler didn't predict certain categories
- Check the bedrock labeler output to ensure it's generating all categories

### Issue: Different number of contracts

**Behavior:**
- The script automatically aligns datasets and only evaluates common contract IDs
- Reports statistics about alignment

**Solution:**
- This is normal if you're evaluating a subset of contracts
- Check the alignment statistics in the output to understand the overlap

## Integration with Workflow

### Complete Evaluation Pipeline

```bash
# Step 1: Generate ground truth CSV from CUAD data
python generate_cuad_csv.py \
    --input-json data/CUADv1.json \
    --output-csv output/ground_truth.csv

# Step 2: Generate predictions using bedrock labeler
python oss120_bedrock_labeler.py \
    --input-csv output/ground_truth.csv \
    --output output/predictions.csv \
    --threads 200

# Step 3: Evaluate predictions
python evaluate_labeler.py \
    --predictions output/predictions.csv \
    --ground-truth output/ground_truth.csv \
    --output evaluation_results/results.json
```

### Batch Evaluation

If you have multiple prediction files to evaluate:

```bash
#!/bin/bash

GROUND_TRUTH="output/ground_truth.csv"
RESULTS_DIR="evaluation_results"

mkdir -p "$RESULTS_DIR"

for pred_file in predictions/*.csv; do
    filename=$(basename "$pred_file" .csv)
    echo "Evaluating $filename..."
    
    python evaluate_labeler.py \
        --predictions "$pred_file" \
        --ground-truth "$GROUND_TRUTH" \
        --output "$RESULTS_DIR/${filename}_results.json"
done

echo "All evaluations complete!"
```

## Advanced Usage

### Programmatic Usage

You can also import and use the evaluation functions in your own Python scripts:

```python
from evaluate_labeler import (
    load_csv_data,
    align_datasets,
    calculate_overall_metrics,
    evaluate_all_categories
)

# Load data
pred_ids, pred_labels = load_csv_data('predictions.csv')
gt_ids, gt_labels = load_csv_data('ground_truth.csv')

# Align datasets
common_ids, aligned_pred, aligned_gt = align_datasets(
    pred_ids, pred_labels, gt_ids, gt_labels
)

# Calculate metrics
overall_metrics = calculate_overall_metrics(aligned_pred, aligned_gt, common_ids)
category_metrics = evaluate_all_categories(aligned_pred, aligned_gt, common_ids)

# Use metrics in your analysis
print(f"Overall F1 Score: {overall_metrics['f1_macro']:.4f}")
```

## Contributing

If you find issues or have suggestions for improvements:
1. Check existing issues in the repository
2. Create a new issue with detailed description
3. Submit a pull request with proposed changes

## License

This evaluation script is part of the CUAD project and follows the same license terms.

## References

- CUAD Dataset: https://www.atticusprojectai.org/cuad
- Scikit-learn Metrics: https://scikit-learn.org/stable/modules/model_evaluation.html
- AWS Bedrock Documentation: https://docs.aws.amazon.com/bedrock/
